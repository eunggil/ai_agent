# ì‹¤ì œ êµ¬í˜„ ë¡œë“œë§µ

> ì–´ë–¤ ìˆœì„œë¡œ ë¬´ì—‡ì„ ë§Œë“¤ì–´ì•¼ í•˜ëŠ”ê°€?

## ëª©ì°¨

1. [êµ¬í˜„ ì² í•™](#1-êµ¬í˜„-ì² í•™)
2. [Phase 0: ì¸í”„ë¼ ê¸°ì´ˆ](#2-phase-0-ì¸í”„ë¼-ê¸°ì´ˆ-1ì£¼)
3. [Phase 1: ë°ì´í„° íŒŒì´í”„ë¼ì¸ MVP](#3-phase-1-ë°ì´í„°-íŒŒì´í”„ë¼ì¸-mvp-2ì£¼)
4. [Phase 2: ë²¡í„° ê²€ìƒ‰ ê¸°ë³¸](#4-phase-2-ë²¡í„°-ê²€ìƒ‰-ê¸°ë³¸-2ì£¼)
5. [Phase 3: ê°„ë‹¨í•œ ì—ì´ì „íŠ¸](#5-phase-3-ê°„ë‹¨í•œ-ì—ì´ì „íŠ¸-2ì£¼)
6. [Phase 4: ê´‘ê³  ë§¤ì¹­ ê¸°ë³¸](#6-phase-4-ê´‘ê³ -ë§¤ì¹­-ê¸°ë³¸-2ì£¼)
7. [Phase 5: PiMS & ì˜¨í†¨ë¡œì§€](#7-phase-5-pims--ì˜¨í†¨ë¡œì§€-3ì£¼)
8. [Phase 6: ê³ ê¸‰ ì—ì´ì „íŠ¸](#8-phase-6-ê³ ê¸‰-ì—ì´ì „íŠ¸-3ì£¼)
9. [Phase 7: í”„ë¡œë•ì…˜ ì¤€ë¹„](#9-phase-7-í”„ë¡œë•ì…˜-ì¤€ë¹„-2ì£¼)
10. [ë³‘ë ¬ ì‘ì—… ê°€ëŠ¥ í•­ëª©](#10-ë³‘ë ¬-ì‘ì—…-ê°€ëŠ¥-í•­ëª©)

---

## 1. êµ¬í˜„ ì² í•™

### í•µì‹¬ ì›ì¹™

1. **ê°€ì¥ ìœ„í—˜í•œ ê²ƒë¶€í„° ê²€ì¦**
   - "ì´ê²Œ ì •ë§ ë ê¹Œ?" â†’ ë¨¼ì € ì¦ëª…
   - LLM ê¸°ë°˜ ìƒì„±, ë²¡í„° ê²€ìƒ‰, ê·¸ë˜í”„ ì œì•½ì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€

2. **End-to-Endë¥¼ ë¹¨ë¦¬**
   - ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ ê°„ë‹¨í•œ ë²„ì „ì„ ë¨¼ì € ì™„ì„±
   - ê° ë‹¨ê³„ë¥¼ ê³ ë„í™”í•˜ëŠ” ê±´ ë‚˜ì¤‘ì—

3. **ë°ì´í„°ë¶€í„°**
   - ì¢‹ì€ AIëŠ” ì¢‹ì€ ë°ì´í„°ì—ì„œ ë‚˜ì˜´
   - í† í°í™”/ì •ê·œí™”ê°€ ì•ˆ ë˜ë©´ ì•„ë¬´ê²ƒë„ ì•ˆ ë¨

4. **ì¸¡ì • ê°€ëŠ¥í•˜ê²Œ**
   - ë§¤ ë‹¨ê³„ë§ˆë‹¤ ì„±ê³µ ì§€í‘œ ì •ì˜
   - ë¡œê·¸/íŠ¸ë ˆì´ìŠ¤ë¥¼ ì²˜ìŒë¶€í„°

---

## 2. Phase 0: ì¸í”„ë¼ ê¸°ì´ˆ (1ì£¼)

### ëª©í‘œ
"ê°œë°œ í™˜ê²½ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìƒíƒœ"

### ì‘ì—… ëª©ë¡

#### 2.1 GCP í”„ë¡œì íŠ¸ ì„¤ì •
```bash
# í”„ë¡œì íŠ¸ ìƒì„±
gcloud projects create addeep-ai-agent-dev

# API í™œì„±í™”
gcloud services enable \
  compute.googleapis.com \
  run.googleapis.com \
  sqladmin.googleapis.com \
  aiplatform.googleapis.com \
  pubsub.googleapis.com \
  firestore.googleapis.com \
  storage.googleapis.com \
  bigquery.googleapis.com
```

#### 2.2 ë¡œì»¬ ê°œë°œ í™˜ê²½
```bash
# ì €ì¥ì†Œ êµ¬ì¡° ìƒì„±
mkdir -p src/{api,core,data,workers,models,utils}
mkdir -p tests/{unit,integration}
mkdir -p infrastructure/{terraform,docker}

# Python í™˜ê²½
python -m venv venv
source venv/bin/activate

# ê¸°ë³¸ ì˜ì¡´ì„±
pip install \
  fastapi uvicorn \
  sqlalchemy psycopg2-binary \
  google-cloud-aiplatform \
  google-cloud-pubsub \
  google-cloud-firestore \
  google-cloud-bigquery \
  langgraph langchain \
  numpy pandas
```

#### 2.3 PostgreSQL + pgvector (ë¡œì»¬)
```bash
# Docker Composeë¡œ ì‹œì‘
docker-compose up -d postgres redis
```

**docker-compose.yml**
```yaml
version: '3.8'

services:
  postgres:
    image: ankane/pgvector:latest
    environment:
      POSTGRES_DB: addeep
      POSTGRES_USER: dev
      POSTGRES_PASSWORD: devpass
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  pgdata:
```

#### 2.4 ì„±ê³µ ê¸°ì¤€
- [ ] GCP í”„ë¡œì íŠ¸ ìƒì„± ì™„ë£Œ
- [ ] ë¡œì»¬ì—ì„œ Python ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥
- [ ] PostgreSQL ì—°ê²° í™•ì¸
- [ ] Redis ì—°ê²° í™•ì¸

---

## 3. Phase 1: ë°ì´í„° íŒŒì´í”„ë¼ì¸ MVP (2ì£¼)

### ëª©í‘œ
"ë°ì´í„°ê°€ ìŒ“ì´ê³  íë¥´ëŠ” ê²ƒ í™•ì¸"

### ìš°ì„ ìˆœìœ„: ğŸ”¥ ìµœìš°ì„ 

**ì´ìœ :**
- ë°ì´í„° ì—†ìœ¼ë©´ ì•„ë¬´ê²ƒë„ ëª» í•¨
- íŒŒì´í”„ë¼ì¸ ê²€ì¦ì´ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦¼
- ë³‘ë ¬ ì‘ì—…ì˜ ê¸°ë°˜

### ì‘ì—… ëª©ë¡

#### 3.1 BigQuery ìŠ¤í‚¤ë§ˆ ìƒì„±
```sql
-- user_behavior_logs
CREATE TABLE `addeep-dev.analytics.user_behavior_logs` (
  event_id STRING NOT NULL,
  user_id STRING NOT NULL,
  event_type STRING NOT NULL,
  content_id STRING,
  timestamp TIMESTAMP NOT NULL,
  metadata JSON
)
PARTITION BY DATE(timestamp)
CLUSTER BY user_id;

-- ai_generation_logs
CREATE TABLE `addeep-dev.analytics.ai_generation_logs` (
  generation_id STRING NOT NULL,
  user_id STRING NOT NULL,
  status STRING NOT NULL,
  cost_usd FLOAT64,
  generation_time_seconds FLOAT64,
  timestamp TIMESTAMP NOT NULL
)
PARTITION BY DATE(timestamp)
CLUSTER BY user_id;
```

#### 3.2 Pub/Sub í† í”½ ìƒì„±
```python
from google.cloud import pubsub_v1

publisher = pubsub_v1.PublisherClient()

topics = [
    'user-events',
    'feed-generation',
    'vector-update',
    'ad-serving'
]

for topic_name in topics:
    topic_path = publisher.topic_path('addeep-dev', topic_name)
    publisher.create_topic(request={"name": topic_path})
```

#### 3.3 ì´ë²¤íŠ¸ Publisher (ê°„ë‹¨í•œ ë²„ì „)
```python
# src/data/pubsub/publisher.py

from google.cloud import pubsub_v1
import json
from typing import Dict, Any

class EventPublisher:
    def __init__(self, project_id: str):
        self.publisher = pubsub_v1.PublisherClient()
        self.project_id = project_id

    async def publish_user_event(self, event: Dict[str, Any]):
        """ì‚¬ìš©ì í–‰ë™ ì´ë²¤íŠ¸ ë°œí–‰"""
        topic_path = self.publisher.topic_path(
            self.project_id,
            'user-events'
        )

        data = json.dumps(event).encode('utf-8')
        future = self.publisher.publish(topic_path, data)

        return future.result()  # ë™ê¸° ëŒ€ê¸° (ê°œë°œ ë‹¨ê³„)

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    publisher = EventPublisher("addeep-dev")

    event = {
        "event_id": "evt_001",
        "user_id": "user_123",
        "event_type": "view",
        "content_id": "cnt_456",
        "timestamp": "2026-02-24T10:00:00Z"
    }

    message_id = publisher.publish_user_event(event)
    print(f"Published: {message_id}")
```

#### 3.4 ê°„ë‹¨í•œ Subscriber (BigQueryë¡œ ì €ì¥)
```python
# src/workers/bigquery_sink.py

from google.cloud import pubsub_v1, bigquery
import json

def callback(message):
    """ë©”ì‹œì§€ë¥¼ BigQueryì— ì €ì¥"""
    data = json.loads(message.data.decode('utf-8'))

    client = bigquery.Client()
    table_id = "addeep-dev.analytics.user_behavior_logs"

    errors = client.insert_rows_json(table_id, [data])

    if not errors:
        message.ack()
    else:
        print(f"Errors: {errors}")
        message.nack()

# Subscriber ì‹œì‘
subscriber = pubsub_v1.SubscriberClient()
subscription_path = subscriber.subscription_path(
    'addeep-dev',
    'user-events-to-bigquery'
)

streaming_pull_future = subscriber.subscribe(
    subscription_path,
    callback=callback
)

print("Listening for messages...")
streaming_pull_future.result()
```

#### 3.5 ì„±ê³µ ê¸°ì¤€
- [ ] ì´ë²¤íŠ¸ ë°œí–‰ â†’ Pub/Sub â†’ BigQuery íë¦„ í™•ì¸
- [ ] BigQueryì—ì„œ ë°ì´í„° ì¡°íšŒ ê°€ëŠ¥
- [ ] ìµœì†Œ 100ê°œ í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸ ì •ìƒ ì²˜ë¦¬

---

## 4. Phase 2: ë²¡í„° ê²€ìƒ‰ ê¸°ë³¸ (2ì£¼)

### ëª©í‘œ
"ìœ ì‚¬ë„ ê²€ìƒ‰ì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸"

### ìš°ì„ ìˆœìœ„: ğŸ”¥ ìµœìš°ì„ 

**ì´ìœ :**
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì˜ í•µì‹¬
- ì˜ì™¸ë¡œ ê¹Œë‹¤ë¡œì›€ (ì„ë² ë”© í’ˆì§ˆ, ê²€ìƒ‰ ì†ë„)

### ì‘ì—… ëª©ë¡

#### 4.1 Vector DB ìŠ¤í‚¤ë§ˆ ìƒì„±
```sql
-- PostgreSQL + pgvector

-- pgvector extension í™œì„±í™”
CREATE EXTENSION IF NOT EXISTS vector;

-- ì½˜í…ì¸  ë²¡í„°
CREATE TABLE content_vectors (
  content_id VARCHAR(255) PRIMARY KEY,
  content_type VARCHAR(50),
  vector vector(768),
  metadata JSONB,
  created_at TIMESTAMP DEFAULT NOW()
);

-- ì¸ë±ìŠ¤ (IVFFlat)
CREATE INDEX idx_content_vectors_ivfflat
ON content_vectors
USING ivfflat (vector vector_cosine_ops)
WITH (lists = 100);
```

#### 4.2 ê°„ë‹¨í•œ ì„ë² ë”© ìƒì„±ê¸°
```python
# src/core/vector/embedder.py

from google.cloud import aiplatform
from typing import List
import numpy as np

class TextEmbedder:
    def __init__(self):
        aiplatform.init(
            project="addeep-dev",
            location="us-central1"
        )
        self.model = aiplatform.TextEmbeddingModel.from_pretrained(
            "text-embedding-004"
        )

    def embed(self, texts: List[str]) -> List[np.ndarray]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = self.model.get_embeddings(texts)
        return [np.array(emb.values) for emb in embeddings]

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    embedder = TextEmbedder()

    texts = [
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”",
        "ë¦½ìŠ¤í‹± ì¶”ì²œí•´ì£¼ì„¸ìš”",
        "í¸ì•ˆí•œ ì˜· ì°¾ì•„ìš”"
    ]

    vectors = embedder.embed(texts)
    print(f"Generated {len(vectors)} embeddings")
    print(f"Dimension: {vectors[0].shape}")
```

#### 4.3 ë²¡í„° ì €ì¥ ë° ê²€ìƒ‰
```python
# src/core/vector/search.py

import psycopg2
import numpy as np
from typing import List, Tuple

class VectorSearch:
    def __init__(self, connection_string: str):
        self.conn = psycopg2.connect(connection_string)

    def insert(self, content_id: str, vector: np.ndarray, metadata: dict):
        """ë²¡í„° ì €ì¥"""
        with self.conn.cursor() as cur:
            cur.execute(
                """
                INSERT INTO content_vectors (content_id, vector, metadata)
                VALUES (%s, %s, %s)
                ON CONFLICT (content_id) DO UPDATE
                SET vector = EXCLUDED.vector, metadata = EXCLUDED.metadata
                """,
                (content_id, vector.tolist(), json.dumps(metadata))
            )
        self.conn.commit()

    def search(self, query_vector: np.ndarray, k: int = 10) -> List[Tuple]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        with self.conn.cursor() as cur:
            cur.execute(
                """
                SELECT
                    content_id,
                    metadata,
                    1 - (vector <=> %s::vector) AS similarity
                FROM content_vectors
                ORDER BY vector <=> %s::vector
                LIMIT %s
                """,
                (query_vector.tolist(), query_vector.tolist(), k)
            )
            return cur.fetchall()

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    search = VectorSearch("postgresql://dev:devpass@localhost/addeep")

    # ìƒ˜í”Œ ë°ì´í„° ì‚½ì…
    embedder = TextEmbedder()

    contents = [
        ("cnt_1", "ë¦½ìŠ¤í‹± ì¶”ì²œ", {"category": "cosmetic"}),
        ("cnt_2", "í‹°ì…”ì¸  ì¶”ì²œ", {"category": "fashion"}),
        ("cnt_3", "ë°˜ì§€ ì¶”ì²œ", {"category": "jewelry"}),
    ]

    for content_id, text, metadata in contents:
        vector = embedder.embed([text])[0]
        search.insert(content_id, vector, metadata)

    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query_text = "í™”ì¥í’ˆ ì¶”ì²œ"
    query_vector = embedder.embed([query_text])[0]

    results = search.search(query_vector, k=3)
    for content_id, metadata, similarity in results:
        print(f"{content_id}: {similarity:.3f}")
```

#### 4.4 ì„±ê³µ ê¸°ì¤€
- [ ] ì„ë² ë”© ìƒì„± ì„±ê³µ
- [ ] ë²¡í„° ì €ì¥ ì„±ê³µ
- [ ] ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ë¯¸ì ìœ¼ë¡œ ë§ìŒ
- [ ] 1000ê°œ ë²¡í„° ê¸°ì¤€ ê²€ìƒ‰ ì†ë„ < 100ms

---

## 5. Phase 3: ê°„ë‹¨í•œ ì—ì´ì „íŠ¸ (2ì£¼)

### ëª©í‘œ
"LangGraphë¡œ ìƒíƒœë¨¸ì‹ ì„ ë§Œë“¤ê³  LLMì´ ê²°ì •í•˜ê²Œ í•˜ê¸°"

### ìš°ì„ ìˆœìœ„: ğŸ”¥ ìµœìš°ì„ 

**ì´ìœ :**
- ì—ì´ì „íŠ¸ê°€ í•µì‹¬ ì°¨ë³„ì 
- ë³µì¡ë„ ë†’ì•„ì„œ ì¼ì° ì‹œì‘

### ì‘ì—… ëª©ë¡

#### 5.1 ê°„ë‹¨í•œ 2-ë…¸ë“œ ê·¸ë˜í”„ (Hello World)
```python
# src/core/ai_agent/simple_agent.py

from langgraph.graph import StateGraph, END
from typing import TypedDict
from google.cloud import aiplatform

class AgentState(TypedDict):
    """ì—ì´ì „íŠ¸ ìƒíƒœ"""
    user_input: str
    analysis: str
    recommendation: str

def analyze_node(state: AgentState) -> AgentState:
    """ì‚¬ìš©ì ì…ë ¥ ë¶„ì„"""
    user_input = state["user_input"]

    # Geminië¡œ ë¶„ì„
    model = aiplatform.GenerativeModel("gemini-1.5-pro")
    prompt = f"ë‹¤ìŒ ìš”ì²­ì„ ë¶„ì„í•˜ì„¸ìš”: {user_input}"

    response = model.generate_content(prompt)
    state["analysis"] = response.text

    return state

def recommend_node(state: AgentState) -> AgentState:
    """ì¶”ì²œ ìƒì„±"""
    analysis = state["analysis"]

    model = aiplatform.GenerativeModel("gemini-1.5-pro")
    prompt = f"ë‹¤ìŒ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”: {analysis}"

    response = model.generate_content(prompt)
    state["recommendation"] = response.text

    return state

# ê·¸ë˜í”„ êµ¬ì„±
workflow = StateGraph(AgentState)
workflow.add_node("analyze", analyze_node)
workflow.add_node("recommend", recommend_node)

workflow.set_entry_point("analyze")
workflow.add_edge("analyze", "recommend")
workflow.add_edge("recommend", END)

# ì»´íŒŒì¼
app = workflow.compile()

# ì‹¤í–‰ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    result = app.invoke({
        "user_input": "ë¦½ìŠ¤í‹± ì¶”ì²œí•´ì£¼ì„¸ìš”",
        "analysis": "",
        "recommendation": ""
    })

    print("Analysis:", result["analysis"])
    print("Recommendation:", result["recommendation"])
```

#### 5.2 ì²´í¬í¬ì¸í„° ì¶”ê°€ (ìƒíƒœ ì €ì¥)
```python
from langgraph.checkpoint.sqlite import SqliteSaver

# ì²´í¬í¬ì¸í„° ìƒì„±
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

# ê·¸ë˜í”„ ì»´íŒŒì¼ ì‹œ ì¶”ê°€
app = workflow.compile(checkpointer=checkpointer)

# Thread IDë¡œ ì„¸ì…˜ ê´€ë¦¬
config = {"configurable": {"thread_id": "user_123"}}

result = app.invoke(
    {"user_input": "ë¦½ìŠ¤í‹± ì¶”ì²œ", "analysis": "", "recommendation": ""},
    config
)
```

#### 5.3 ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì¶”ê°€
```python
def should_ask_more(state: AgentState) -> str:
    """ë” ì§ˆë¬¸í• ì§€ ê²°ì •"""
    analysis = state["analysis"]

    if "ë¶ˆí™•ì‹¤" in analysis or "ì •ë³´ ë¶€ì¡±" in analysis:
        return "ask_more"
    return "recommend"

workflow.add_conditional_edges(
    "analyze",
    should_ask_more,
    {
        "ask_more": "ask_more_info",
        "recommend": "recommend"
    }
)
```

#### 5.4 ì„±ê³µ ê¸°ì¤€
- [ ] 2-ë…¸ë“œ ê·¸ë˜í”„ ì‹¤í–‰ ì„±ê³µ
- [ ] ìƒíƒœê°€ ì²´í¬í¬ì¸í„°ì— ì €ì¥ë¨
- [ ] ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì‘ë™
- [ ] Thread ê¸°ë°˜ ì„¸ì…˜ ê´€ë¦¬ í™•ì¸

---

## 6. Phase 4: ê´‘ê³  ë§¤ì¹­ ê¸°ë³¸ (2ì£¼)

### ëª©í‘œ
"ë²¡í„° ê²€ìƒ‰ + ê°„ë‹¨í•œ ë­í‚¹ìœ¼ë¡œ ê´‘ê³  ì„ íƒ"

### ìš°ì„ ìˆœìœ„: ğŸŸ¡ ì¤‘ìš”

### ì‘ì—… ëª©ë¡

#### 6.1 ê´‘ê³  ë°ì´í„° ì¤€ë¹„
```sql
-- ê´‘ê³  ë²¡í„° í…Œì´ë¸”
CREATE TABLE ad_vectors (
  ad_id VARCHAR(255) PRIMARY KEY,
  campaign_id VARCHAR(255),
  vector vector(768),
  metadata JSONB,
  bid_amount DECIMAL(6, 2),
  budget_remaining DECIMAL(10, 2),
  status VARCHAR(20) DEFAULT 'active'
);

-- ìƒ˜í”Œ ê´‘ê³  ë°ì´í„°
INSERT INTO ad_vectors (ad_id, campaign_id, vector, metadata, bid_amount, budget_remaining)
VALUES
  ('ad_001', 'camp_001', (SELECT vector FROM content_vectors WHERE content_id = 'cnt_1'),
   '{"product": "ë¦½ìŠ¤í‹±", "brand": "ë¸Œëœë“œA"}', 2.5, 1000.0),
  ('ad_002', 'camp_002', (SELECT vector FROM content_vectors WHERE content_id = 'cnt_2'),
   '{"product": "í‹°ì…”ì¸ ", "brand": "ë¸Œëœë“œB"}', 1.8, 1500.0);
```

#### 6.2 ê°„ë‹¨í•œ ê´‘ê³  ë§¤ì¹­
```python
# src/core/ads/simple_matcher.py

class SimpleAdMatcher:
    def __init__(self, vector_search: VectorSearch):
        self.search = vector_search

    def match_ads(self, content_vector: np.ndarray, k: int = 5) -> List[dict]:
        """ì½˜í…ì¸ ì— ë§ëŠ” ê´‘ê³  ì°¾ê¸°"""

        # 1. ë²¡í„° ê²€ìƒ‰
        with self.search.conn.cursor() as cur:
            cur.execute(
                """
                SELECT
                    ad_id,
                    campaign_id,
                    metadata,
                    bid_amount,
                    budget_remaining,
                    1 - (vector <=> %s::vector) AS similarity
                FROM ad_vectors
                WHERE status = 'active' AND budget_remaining > 0
                ORDER BY vector <=> %s::vector
                LIMIT %s
                """,
                (content_vector.tolist(), content_vector.tolist(), k)
            )
            results = cur.fetchall()

        # 2. ê°„ë‹¨í•œ ìŠ¤ì½”ì–´ë§ (similarity Ã— bid)
        scored = []
        for row in results:
            ad_id, campaign_id, metadata, bid, budget, similarity = row
            score = similarity * (bid / 10.0)  # ì •ê·œí™”

            scored.append({
                "ad_id": ad_id,
                "campaign_id": campaign_id,
                "metadata": metadata,
                "similarity": similarity,
                "bid": bid,
                "score": score
            })

        # 3. ì •ë ¬
        scored.sort(key=lambda x: x["score"], reverse=True)

        return scored

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    search = VectorSearch("postgresql://dev:devpass@localhost/addeep")
    matcher = SimpleAdMatcher(search)

    embedder = TextEmbedder()
    query_vector = embedder.embed(["ë¦½ìŠ¤í‹± ì¶”ì²œ"])[0]

    matched_ads = matcher.match_ads(query_vector, k=3)

    for ad in matched_ads:
        print(f"{ad['ad_id']}: score={ad['score']:.3f}, similarity={ad['similarity']:.3f}")
```

#### 6.3 ì„±ê³µ ê¸°ì¤€
- [ ] ê´‘ê³  ë²¡í„° ì €ì¥ ì„±ê³µ
- [ ] ì½˜í…ì¸  ê¸°ë°˜ ê´‘ê³  ê²€ìƒ‰ ì„±ê³µ
- [ ] ìŠ¤ì½”ì–´ë§ ê²°ê³¼ê°€ í•©ë¦¬ì 
- [ ] 3ê°œ ì´ìƒ ê´‘ê³  í›„ë³´ ë°˜í™˜

---

## 7. Phase 5: PiMS & ì˜¨í†¨ë¡œì§€ (3ì£¼)

### ëª©í‘œ
"ìƒí’ˆ ë°ì´í„°ë¥¼ AIê°€ ì´í•´í•˜ëŠ” í˜•íƒœë¡œ"

### ìš°ì„ ìˆœìœ„: ğŸŸ¡ ì¤‘ìš” (ë³‘ë ¬ ê°€ëŠ¥)

### ì‘ì—… ëª©ë¡

#### 7.1 ì˜¨í†¨ë¡œì§€ v0 ì½”ë“œ í…Œì´ë¸”
```python
# src/models/ontology.py

from enum import Enum

class Category(str, Enum):
    # Fashion
    FASHION_TOP = "FASHION>TOP"
    FASHION_BOTTOM = "FASHION>BOTTOM"
    FASHION_OUTER = "FASHION>OUTER"

    # Cosmetic
    COSMETIC_LIP = "COSMETIC>LIP"
    COSMETIC_BASE = "COSMETIC>BASE"
    COSMETIC_EYE = "COSMETIC>EYE"

    # Jewelry
    JEWELRY_RING = "JEWELRY>RING"
    JEWELRY_NECKLACE = "JEWELRY>NECKLACE"

class Attribute(str, Enum):
    # Color
    COLOR_BLACK = "BLACK"
    COLOR_WHITE = "WHITE"
    COLOR_BEIGE = "BEIGE"

    # Fit (Fashion)
    FIT_SLIM = "SLIM"
    FIT_REGULAR = "REGULAR"
    FIT_OVERSIZE = "OVERSIZE"

    # Finish (Cosmetic)
    FINISH_MATTE = "MATTE"
    FINISH_SATIN = "SATIN"
    FINISH_GLOSSY = "GLOSSY"

# ì •ê·œí™” ì‚¬ì „
NORMALIZATION_DICT = {
    "color": {
        "ì˜¤í”„í™”ì´íŠ¸": "BEIGE",
        "ì•„ì´ë³´ë¦¬": "BEIGE",
        "í¬ë¦¼": "BEIGE",
        "ê²€ì •": "BLACK",
        "í°ìƒ‰": "WHITE",
    },
    "fit": {
        "ìŠ¬ë¦¼": "SLIM",
        "ê¸°ë³¸": "REGULAR",
        "ë ˆê·¤ëŸ¬": "REGULAR",
        "ì˜¤ë²„": "OVERSIZE",
        "ë£¨ì¦ˆ": "OVERSIZE",
    },
    "finish": {
        "ë§¤íŠ¸": "MATTE",
        "ë¬´ê´‘": "MATTE",
        "ê¸€ë¡œì‹œ": "GLOSSY",
        "ìœ¤ê´‘": "GLOSSY",
    }
}
```

#### 7.2 Product Token ìƒì„±ê¸°
```python
# src/core/pims/token_generator.py

from typing import Dict
import json

class ProductTokenGenerator:
    def __init__(self, llm_client):
        self.llm = llm_client

    def extract_from_description(self, description: str, images: List[str] = None) -> dict:
        """ìƒí’ˆ ì„¤ëª…ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""

        prompt = f"""
ë‹¤ìŒ ìƒí’ˆ ì„¤ëª…ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì„¤ëª…: {description}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥:
{{
  "category_path": "COSMETIC>LIP>LIPSTICK",
  "attributes": {{
    "color_family": "BEIGE",
    "finish": "SATIN",
    ...
  }},
  "style_tags": ["natural", "daily"],
  "claims_detected": ["moisturizing"]
}}
"""

        response = self.llm.generate_content(prompt)
        extracted = json.loads(response.text)

        return extracted

    def normalize_attributes(self, attributes: dict) -> dict:
        """ë™ì˜ì–´ë¥¼ í‘œì¤€ ì½”ë“œë¡œ ë§¤í•‘"""
        normalized = {}

        for key, value in attributes.items():
            if key in NORMALIZATION_DICT:
                normalized[key] = NORMALIZATION_DICT[key].get(
                    value.lower(),
                    value
                )
            else:
                normalized[key] = value

        return normalized

    def create_product_token(self, product_id: str, description: str) -> dict:
        """Product Token ìƒì„±"""

        # 1. ì¶”ì¶œ
        extracted = self.extract_from_description(description)

        # 2. ì •ê·œí™”
        normalized_attrs = self.normalize_attributes(extracted["attributes"])

        # 3. í† í° êµ¬ì„±
        token = {
            "product_id": product_id,
            "version": 1,
            "source": "AI_AGENT",
            "normalized": {
                "category_path": extracted["category_path"],
                "attributes": normalized_attrs,
                "style_tags": extracted["style_tags"]
            },
            "constraints": {
                "claims_allowed": extracted.get("claims_detected", []),
                "claims_forbidden": []  # ì •ì±…ì—ì„œ ë¡œë“œ
            }
        }

        return token

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    generator = ProductTokenGenerator(llm_client)

    description = "ì´‰ì´‰í•œ ë§¤íŠ¸ ë¦½ìŠ¤í‹±, ìì—°ìŠ¤ëŸ¬ìš´ ëˆ„ë“œ ë² ì´ì§€ ì»¬ëŸ¬"

    token = generator.create_product_token("P001", description)
    print(json.dumps(token, indent=2, ensure_ascii=False))
```

#### 7.3 ì„±ê³µ ê¸°ì¤€
- [ ] ì˜¨í†¨ë¡œì§€ ì½”ë“œ í…Œì´ë¸” ì •ì˜
- [ ] LLM ì¶”ì¶œ ì„±ê³µë¥  > 80%
- [ ] ì •ê·œí™” ì»¤ë²„ë¦¬ì§€ > 70%
- [ ] Product Token ìƒì„± ì„±ê³µ

---

## 8. Phase 6: ê³ ê¸‰ ì—ì´ì „íŠ¸ (3ì£¼)

### ëª©í‘œ
"ì‹¤ì œ ê´‘ê³ ê²°í•© ìƒì„± ì—ì´ì „íŠ¸ ì™„ì„±"

### ìš°ì„ ìˆœìœ„: ğŸŸ¢ ê³ ë„í™”

### ì‘ì—… ëª©ë¡

#### 8.1 9ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
```python
# src/core/ai_agent/ad_combination_agent.py

class AdCombinationAgent:
    """ê´‘ê³ ê²°í•© ì½˜í…ì¸  ìƒì„± ì—ì´ì „íŠ¸"""

    def __init__(self):
        self.workflow = self._build_workflow()

    def _build_workflow(self):
        """9ë‹¨ê³„ ê·¸ë˜í”„ êµ¬ì¶•"""

        workflow = StateGraph(AgentState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("ingest", self.ingest_node)
        workflow.add_node("safety_precheck", self.safety_precheck_node)
        workflow.add_node("hybrid_retrieve", self.hybrid_retrieve_node)
        workflow.add_node("ad_ranking", self.ad_ranking_node)
        workflow.add_node("act_plan", self.act_plan_node)
        workflow.add_node("generate", self.generate_node)
        workflow.add_node("post_check", self.post_check_node)
        workflow.add_node("publish", self.publish_node)

        # ì—£ì§€ ì •ì˜
        workflow.set_entry_point("ingest")
        workflow.add_edge("ingest", "safety_precheck")

        # ì¡°ê±´ë¶€ ë¶„ê¸°: Safety ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
        workflow.add_conditional_edges(
            "safety_precheck",
            lambda state: "proceed" if state["safety_passed"] else "abort",
            {
                "proceed": "hybrid_retrieve",
                "abort": END
            }
        )

        workflow.add_edge("hybrid_retrieve", "ad_ranking")
        workflow.add_edge("ad_ranking", "act_plan")
        workflow.add_edge("act_plan", "generate")
        workflow.add_edge("generate", "post_check")

        # ì¡°ê±´ë¶€ ë¶„ê¸°: Post-check ì‹¤íŒ¨ ì‹œ ì¬ìƒì„±
        workflow.add_conditional_edges(
            "post_check",
            lambda state: "publish" if state["post_check_passed"] else "generate",
            {
                "publish": "publish",
                "generate": "generate"  # ì¬ì‹œë„
            }
        )

        workflow.add_edge("publish", END)

        return workflow.compile(checkpointer=checkpointer)
```

#### 8.2 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Vector + Graph)
```python
async def hybrid_retrieve_node(self, state: AgentState) -> AgentState:
    """Vector ê²€ìƒ‰ + Graph ì œì•½"""

    content_vector = state["content_vector"]

    # 1. Vector ê²€ìƒ‰ (ë„“ê²Œ)
    vector_candidates = await self.vector_search.search(
        content_vector,
        k=20
    )

    # 2. Graph ì œì•½ í•„í„°ë§ (ì¢ê²Œ)
    eligible_candidates = []

    for candidate in vector_candidates:
        # Cypher ì¿¼ë¦¬ë¡œ ì ê²©ì„± ì²´í¬
        is_eligible = await self.graph_db.execute(
            """
            MATCH (p:Product {product_id: $product_id})-[:BELONGS_TO]->(b:Brand)
            WHERE p.status = 'ACTIVE' AND b.status = 'ACTIVE'
            RETURN count(*) > 0 AS eligible
            """,
            {"product_id": candidate["product_id"]}
        )

        if is_eligible:
            eligible_candidates.append(candidate)

    state["ad_candidates"] = eligible_candidates
    return state
```

#### 8.3 ë¸Œëœë“œ ì•ˆì „ ì¥ì¹˜
```python
async def post_check_node(self, state: AgentState) -> AgentState:
    """ìƒì„± ê²°ê³¼ ê²€ì¦ (3ì¤‘ ë°©ì–´)"""

    generated_content = state["generated_content"]
    brand_spec = state["brand_spec"]

    # 1. Template Lock (ì´ë¯¸ ìƒì„± ì‹œ ì ìš©ë¨)

    # 2. Vision Post-check
    violations = await self.vision_checker.check(
        generated_content.image,
        brand_spec.logo_spec
    )

    if violations:
        # 3. Repair Loop
        repaired = await self.repair_loop(
            generated_content,
            violations,
            max_retries=3
        )

        if repaired:
            state["generated_content"] = repaired
            state["post_check_passed"] = True
        else:
            state["post_check_passed"] = False
            state["post_check_error"] = "Repair failed"
    else:
        state["post_check_passed"] = True

    return state
```

#### 8.4 ì„±ê³µ ê¸°ì¤€
- [ ] 9ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì™„ì„±
- [ ] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‘ë™
- [ ] ë¸Œëœë“œ ì•ˆì „ ì¥ì¹˜ ì‘ë™
- [ ] End-to-End í…ŒìŠ¤íŠ¸ ì„±ê³µ

---

## 9. Phase 7: í”„ë¡œë•ì…˜ ì¤€ë¹„ (2ì£¼)

### ëª©í‘œ
"ì‹¤ì œ ì‚¬ìš©ìì—ê²Œ ì„œë¹„ìŠ¤ ê°€ëŠ¥í•œ ìƒíƒœ"

### ìš°ì„ ìˆœìœ„: ğŸ”´ í•„ìˆ˜

### ì‘ì—… ëª©ë¡

#### 9.1 ëª¨ë‹ˆí„°ë§
```python
# src/utils/metrics.py

from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
generation_requests = Counter(
    'generation_requests_total',
    'Total generation requests',
    ['status']
)

generation_duration = Histogram(
    'generation_duration_seconds',
    'Generation duration'
)

generation_cost = Histogram(
    'generation_cost_usd',
    'Generation cost in USD'
)

# ì‚¬ìš©
with generation_duration.time():
    result = await agent.generate(request)

generation_cost.observe(result.cost)
generation_requests.labels(status='success').inc()
```

#### 9.2 ë¡œê¹… ë° íŠ¸ë ˆì´ì‹±
```python
# src/utils/logging.py

import logging
from google.cloud import logging as cloud_logging

client = cloud_logging.Client()
client.setup_logging()

logger = logging.getLogger(__name__)

# êµ¬ì¡°í™”ëœ ë¡œê¹…
def log_generation(request_id, user_id, status, duration, cost):
    logger.info(
        "Generation completed",
        extra={
            "request_id": request_id,
            "user_id": user_id,
            "status": status,
            "duration_seconds": duration,
            "cost_usd": cost,
            "labels": {
                "component": "ai_agent",
                "environment": "production"
            }
        }
    )
```

#### 9.3 ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
async def generate_with_retry(request):
    """ì¬ì‹œë„ ë¡œì§"""
    try:
        return await agent.generate(request)
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        raise
```

#### 9.4 ì„±ê³µ ê¸°ì¤€
- [ ] Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] Cloud Logging ì—°ë™
- [ ] ì¬ì‹œë„ ë¡œì§ ì‘ë™
- [ ] ì—ëŸ¬ ì•Œë¦¼ ì„¤ì •

---

## 10. ë³‘ë ¬ ì‘ì—… ê°€ëŠ¥ í•­ëª©

ë‹¤ìŒ ì‘ì—…ë“¤ì€ ë³‘ë ¬ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

### íŒ€ A: ë°ì´í„° & ì¸í”„ë¼
- Phase 1: ë°ì´í„° íŒŒì´í”„ë¼ì¸
- Phase 2: ë²¡í„° ê²€ìƒ‰
- Phase 7: ëª¨ë‹ˆí„°ë§

### íŒ€ B: AI & ì—ì´ì „íŠ¸
- Phase 3: ê°„ë‹¨í•œ ì—ì´ì „íŠ¸
- Phase 6: ê³ ê¸‰ ì—ì´ì „íŠ¸

### íŒ€ C: PiMS & ì˜¨í†¨ë¡œì§€
- Phase 5: PiMS & ì˜¨í†¨ë¡œì§€
- ê·¸ë˜í”„ DB ì„¤ê³„

### íŒ€ D: ê´‘ê³  ì‹œìŠ¤í…œ
- Phase 4: ê´‘ê³  ë§¤ì¹­
- ê´‘ê³  ë­í‚¹ ê³ ë„í™”

---

## ìš”ì•½: ì¶”ì²œ ìˆœì„œ

### ìµœìš°ì„  (1-4ì£¼)
```
Week 1: Phase 0 (ì¸í”„ë¼ ê¸°ì´ˆ)
Week 2-3: Phase 1 (ë°ì´í„° íŒŒì´í”„ë¼ì¸) + Phase 2 (ë²¡í„° ê²€ìƒ‰)
Week 4-5: Phase 3 (ê°„ë‹¨í•œ ì—ì´ì „íŠ¸)
```

### ì¤‘ìš” (5-9ì£¼)
```
Week 6-7: Phase 4 (ê´‘ê³  ë§¤ì¹­)
Week 8-10: Phase 5 (PiMS & ì˜¨í†¨ë¡œì§€)
```

### ê³ ë„í™” (10-15ì£¼)
```
Week 11-13: Phase 6 (ê³ ê¸‰ ì—ì´ì „íŠ¸)
Week 14-15: Phase 7 (í”„ë¡œë•ì…˜ ì¤€ë¹„)
```

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹œì‘ ì „
- [ ] GCP í”„ë¡œì íŠ¸ ìƒì„±
- [ ] íŒ€ ì—­í•  ë¶„ë‹´
- [ ] ê°œë°œ í™˜ê²½ ì„¤ì •

### ë§¤ Phase í›„
- [ ] ì„±ê³µ ê¸°ì¤€ ë‹¬ì„± í™•ì¸
- [ ] ì½”ë“œ ë¦¬ë·°
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸
- [ ] ë°ëª¨ ì¤€ë¹„

### MVP ì™„ì„± (Phase 1-3)
- [ ] ë°ì´í„° íë¦„ í™•ì¸
- [ ] ë²¡í„° ê²€ìƒ‰ ì‘ë™
- [ ] ì—ì´ì „íŠ¸ ì‹¤í–‰ ì„±ê³µ
- [ ] íŒ€ ë°ëª¨

### í”„ë¡œë•ì…˜ ì¤€ë¹„ (Phase 7)
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ë³´ì•ˆ ì ê²€
- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

---

**ë¬¸ì„œ ë²„ì „:** 1.0
**ìµœì¢… ìˆ˜ì •ì¼:** 2026-02-24
**ì‘ì„±ì:** Implementation Team
